# ===== Ollama endpoint =====
# Leave empty to use the default (http://localhost:11434) and allow your
# WSL auto-detection to rewrite localhost -> Windows host IP when needed.
OLLAMA_URL=

# Disable WSL host-IP auto-detection (keep localhost as-is).
# Set this to 1 if your detection ever picks a bad IP or you run Ollama inside WSL.
OLLAMA_SKIP_WSL_IP_DETECT=0


# ===== Model selection =====
# Default model used by investigate + explain (and as fallback everywhere).
INVESTIGATE_MODEL=llama3.1:8b

# Per-tool overrides (leave empty to use INVESTIGATE_MODEL)
AI_COMMIT_MODEL=
SMART_PARSE_MODEL=

# ===== Context warning =====
# This is a WARNING threshold (characters), not an actual model limit.
LLM_SOFT_CONTEXT_LIMIT=40000
