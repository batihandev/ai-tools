# ===== Ollama endpoint =====
# Leave empty to use the default (http://localhost:11434) and allow your
# WSL auto-detection to rewrite localhost -> Windows host IP when needed.
OLLAMA_URL=

# Disable WSL host-IP auto-detection (keep localhost as-is).
# Set this to 1 if your detection ever picks a bad IP or you run Ollama inside WSL.
OLLAMA_SKIP_WSL_IP_DETECT=0


# ===== Model selection =====
# 1. Global Overrides (Takes precedence over EVERYTHING if set)
# OVERRIDE_LLM_MODEL=
# OVERRIDE_VLM_MODEL=

# 2. Default model used by investigate + explain.
INVESTIGATE_MODEL=llama3.1:8b

# 3. Per-tool overrides
# AI_COMMIT_MODEL=
# SMART_PARSE_MODEL=
# ENGLISH_TEACHER_MODEL=


# ===== Context warning =====
# This is a WARNING threshold (characters), not an actual model limit.
LLM_SOFT_CONTEXT_LIMIT=40000


# ===== Investigate / Explain =====
# Path to the shared log file.
# Defaults to inheriting from repo/logs/investigate-last.log
# INVESTIGATE_LOG=


# ===== Vision / screenshots (screen_explain) =====
SCREENSHOT_DIR="/mnt/c/Users/xxx/OneDrive/Pictures"
VLM_MODEL=qwen2.5vl:7b

# Debugging & Verbosity
VLM_DEBUG=0
VLM_VERBOSE=1
VLM_QUIET=0
VLM_USE_RICH=1

# Advanced VLM Tuning
# VLM_TIMEOUT=180
# VLM_MAX_CTX=8192
# VLM_JPEG_QUALITY=85
# VLM_SNAP_MULT=32
# VLM_SNAP_MIN=64
# VLM_NUM_BATCH=128


# ===== Speech-to-text (faster-whisper) =====
WHISPER_MODEL=small
WHISPER_LANG=en

# cpu is simplest. If you have CUDA working inside WSL2, set cuda.
WHISPER_DEVICE=cpu

# cpu-friendly: int8
# cuda-friendly: int8_float16 or float16
WHISPER_COMPUTE_TYPE=int8


# ===== English teacher =====
ENGLISH_TEACHER_NUM_CTX=4096
ENGLISH_TEACHER_TIMEOUT=60
ENGLISH_TEACHER_MODE=coach


# ===== FastAPI server =====
API_HOST=127.0.0.1
API_PORT=8008


# ===== Postgres =====
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/ai_scripts