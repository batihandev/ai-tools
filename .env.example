# ===== Ollama endpoint =====
# Leave empty to use the default (http://localhost:11434) and allow your
# WSL auto-detection to rewrite localhost -> Windows host IP when needed.
OLLAMA_URL=

# Disable WSL host-IP auto-detection (keep localhost as-is).
# Set this to 1 if your detection ever picks a bad IP or you run Ollama inside WSL.
OLLAMA_SKIP_WSL_IP_DETECT=0


# ===== Model selection =====
# Default model used by investigate + explain (and as fallback everywhere).
INVESTIGATE_MODEL=llama3.1:8b

# Per-tool overrides (leave empty to use INVESTIGATE_MODEL)
# AI_COMMIT_MODEL=
# SMART_PARSE_MODEL=


# ===== Context warning =====
# This is a WARNING threshold (characters), not an actual model limit.
LLM_SOFT_CONTEXT_LIMIT=40000


# ===== Vision / screenshots =====
SCREENSHOT_DIR="/mnt/c/Users/xxx/OneDrive/Pictures"
VLM_MODEL=qwen2.5vl:7b
VLM_DEBUG=1


# ===== Speech-to-text (faster-whisper) =====
WHISPER_MODEL=small
WHISPER_LANG=en

# cpu is simplest. If you have CUDA working inside WSL2, set cuda.
WHISPER_DEVICE=cpu

# cpu-friendly: int8
# cuda-friendly: int8_float16 or float16
WHISPER_COMPUTE_TYPE=int8

# ===== FastAPI server =====
API_HOST=127.0.0.1
API_PORT=8008

# ===== Postgres =====
DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/ai_scripts


# ===== English teacher =====
ENGLISH_TEACHER_MODEL=llama3.1:8b
ENGLISH_TEACHER_NUM_CTX=4096
ENGLISH_TEACHER_TIMEOUT=60
ENGLISH_TEACHER_MODE=coach